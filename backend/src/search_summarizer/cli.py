import argparse
import json
import sys
import os
import time
import requests
from bs4 import BeautifulSoup
from ddgs import DDGS
from groq import Groq
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# ==============================================================================
# Utility Functions
# ==============================================================================

def mock_summarize(query, error_message="Chave da API Groq nÃ£o detectada."):
    return {
        "aviso": f"Resumo simulado para '{query}'. Motivo: {error_message}",
        "detalhes": [
            "A funcionalidade de busca estÃ¡ operando.",
            "Verifique os logs do servidor para mais detalhes sobre o erro."
        ],
    }

def search_duckduckgo(query, max_results=8, region='br-pt'):
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=max_results, region=region))
        return results
    except Exception as e:
        print(f"Erro na busca DuckDuckGo: {e}", file=sys.stderr)
        return []

def scrape_page(url, timeout=5):
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
            
        text = soup.get_text()
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text[:5000]
    except Exception:
        return ""

def _parse_retry_wait(error_msg):
    """Extract wait time in seconds from Groq rate limit error message."""
    import re as _re
    match = _re.search(r"try again in (\d+)m([\d.]+)s", error_msg)
    if match:
        return int(match.group(1)) * 60 + int(float(match.group(2)))
    match = _re.search(r"try again in ([\d.]+)s", error_msg)
    if match:
        return int(float(match.group(1)))
    return 0

def call_groq(api_key, prompt, temperature=0.5, max_retries=4, model=None, force_json=True):
    """Generic Groq API call with exponential backoff and multi-model fallback.
    
    Backoff strategy: 2s â†’ 4s â†’ 8s â†’ 10s (capped) on rate limits.
    Each model on Groq has independent daily token limits, so switching models
    gives access to more tokens when one model's quota is exhausted.
    """
    client = Groq(api_key=api_key)
    
    # Full fallback chain with separate TPD quotas
    all_models = [
        "llama-3.3-70b-versatile",   # 100K TPD (best quality)
        "llama-3.1-8b-instant",      # 500K TPD (fast)
        "llama3-8b-8192",            # separate quota (Llama 3 original)
        "llama3-70b-8192",           # separate quota (Llama 3 original 70b)
    ]
    
    # If specific model requested, put it first but keep fallbacks
    if model:
        models = [model] + [m for m in all_models if m != model]
    else:
        models = all_models

    for mi, current_model in enumerate(models):
        for attempt in range(max_retries):
            try:
                completion_params = {
                    "messages": [{"role": "user", "content": prompt}],
                    "model": current_model,
                    "temperature": temperature,
                }
                
                if force_json:
                    completion_params["response_format"] = {"type": "json_object"}
                
                completion = client.chat.completions.create(**completion_params)
                
                if mi > 0:
                    print(f"  âš¡ Usando modelo fallback: {current_model}", file=sys.stderr)
                
                content = completion.choices[0].message.content
                
                # Return parsed JSON if force_json, otherwise raw text
                return json.loads(content) if force_json else content
                
            except Exception as e:
                error_msg = str(e)
                is_rate_limit = "429" in error_msg
                is_tpd = "tokens per day" in error_msg.lower() or "TPD" in error_msg
                
                is_model_error = "400" in error_msg and ("does not exist" in error_msg or "not supported" in error_msg or "decommissioned" in error_msg or "The model" in error_msg)

                if is_model_error and mi < len(models) - 1:
                    print(f"  âš ï¸ Modelo {current_model} indisponÃ­vel. Trocando...", file=sys.stderr)
                    break

                if is_rate_limit and is_tpd:
                    retry_secs = _parse_retry_wait(error_msg)
                    if attempt < max_retries - 1 and retry_secs <= 30:
                        # Exponential backoff: min(2^(attempt+1), 10) seconds
                        backoff = min(2 ** (attempt + 1), 10)
                        wait = max(backoff, retry_secs) if retry_secs else backoff
                        print(f"  â³ Rate limit (TPD) em {current_model}. Backoff {wait}s... ({attempt+1}/{max_retries})", file=sys.stderr)
                        time.sleep(wait)
                        continue
                    elif mi < len(models) - 1:
                        print(f"  ðŸ”„ TPD esgotado em {current_model} (espera: {retry_secs}s). Trocando modelo...", file=sys.stderr)
                        break
                    raise
                elif is_rate_limit and attempt < max_retries - 1:
                    # Exponential backoff: 2s, 4s, 8s, 10s (capped)
                    backoff = min(2 ** (attempt + 1), 10)
                    print(f"  â³ Rate limit ({current_model}). Backoff {backoff}s... ({attempt+1}/{max_retries})", file=sys.stderr)
                    time.sleep(backoff)
                    continue
                elif is_rate_limit and mi < len(models) - 1:
                    print(f"  ðŸ”„ Rate limit esgotado em {current_model}. Trocando modelo...", file=sys.stderr)
                    break
                raise
    raise Exception("Todos os modelos esgotaram o rate limit diÃ¡rio. Tente novamente mais tarde.")

# ==============================================================================
# Simple Search Mode (original)
# ==============================================================================

def summarize_with_groq(text, query, api_key):
    if not api_key:
        return mock_summarize(query, "Chave da API nÃ£o fornecida.")

    try:
        prompt = f"""
        VocÃª Ã© um assistente de pesquisa avanÃ§ado. Seu objetivo Ã© criar um resumo estruturado e abrangente sobre "{query}" com base no texto fornecido abaixo.
        
        Regras de Resposta:
        1. Retorne APENAS um JSON vÃ¡lido. NÃ£o use blocos de cÃ³digo markdown.
        2. A estrutura do JSON deve ser hierÃ¡rquica e semÃ¢ntica.
        3. Use chaves em PortuguÃªs.
        4. O JSON deve conter uma visÃ£o geral, principais pontos, detalhes tÃ©cnicos (se aplicÃ¡vel), e controvÃ©rsias ou opiniÃµes diversas (se houver).
        5. Seja direto e informativo.
        
        Texto Base:
        {text[:25000]} 
        """
        return call_groq(api_key, prompt)
    except Exception as e:
        error_msg = str(e)
        print(f"Erro na API Groq: {error_msg}", file=sys.stderr)
        if "401" in error_msg:
            return mock_summarize(query, "Chave da API invÃ¡lida (401).")
        if "400" in error_msg:
            return mock_summarize(query, "RequisiÃ§Ã£o invÃ¡lida (400).")
        if "429" in error_msg:
            return mock_summarize(query, "Muitas requisiÃ§Ãµes (429). Aguarde.")
        return mock_summarize(query, f"Erro na API: {error_msg}")

def run_simple_search(args):
    """Original simple search mode."""
    results = search_duckduckgo(args.query, args.max_results, args.region)
    
    if not results:
        return {"structured": {"erro": "Nenhum resultado encontrado"}, "sources": []}

    aggregated_text = ""
    sources = []
    
    for i, result in enumerate(results):
        sources.append(result.get('href'))
        snippet = result.get('body', '')
        aggregated_text += f"Fonte {i+1} ({result.get('title')}): {snippet}\n"
        
        if i < args.max_pages and not args.no_groq:
            content = scrape_page(result.get('href'))
            if content:
                aggregated_text += f"ConteÃºdo extra da Fonte {i+1}: {content}\n"
    
    groq_api_key = os.environ.get("GROQ_API_KEY")
    
    if args.no_groq:
        summary_data = {"aviso": "Resumo via Groq desativado."}
    else:
        summary_data = summarize_with_groq(aggregated_text, args.query, groq_api_key)

    return {"structured": summary_data, "sources": sources}

# ==============================================================================
# Business Intelligence Mode
# ==============================================================================

BUSINESS_CATEGORIES = [
    {
        "id": "mercado",
        "nome": "Panorama do Mercado",
        "icone": "ðŸ“Š",
        "cor": "#10b981",
        "foco": "tamanho do mercado em R$, taxa de crescimento (CAGR), tendÃªncias de consumo, sazonalidade, oportunidades de nicho",
        "nao_falar": "NÃƒO repita o que o usuÃ¡rio jÃ¡ disse sobre o prÃ³prio negÃ³cio. Foque em dados EXTERNOS que ele ainda nÃ£o sabe."
    },
    {
        "id": "concorrentes",
        "nome": "Mapa de Concorrentes",
        "icone": "ðŸŽ¯",
        "cor": "#f59e0b",
        "foco": "nomes dos concorrentes diretos na regiÃ£o, diferenciais de cada um, pontos fracos explorÃ¡veis, comparaÃ§Ã£o de serviÃ§os",
        "nao_falar": "NÃƒO liste apenas nomes. Para CADA concorrente cite: o que ele faz bem, o que faz mal, e como o usuÃ¡rio pode superÃ¡-lo."
    },
    {
        "id": "publico_alvo",
        "nome": "Quem Compra de VocÃª",
        "icone": "ðŸ‘¥",
        "cor": "#8b5cf6",
        "foco": "nomes e perfis de EMPRESAS COMPRADORAS (nÃ£o fornecedoras) do produto, setores que mais consomem, critÃ©rios que esses compradores usam para escolher fornecedor, onde encontrÃ¡-los",
        "nao_falar": "NÃƒO busque empresas do mesmo ramo do usuÃ¡rio. Busque empresas que COMPRAM o produto do usuÃ¡rio (seus potenciais clientes)."
    },
    {
        "id": "como_vender",
        "nome": "Como Prospectar Clientes",
        "icone": "ðŸ’°",
        "cor": "#ef4444",
        "foco": "tÃ©cnicas concretas de prospecÃ§Ã£o para esse nicho, scripts de abordagem, canais que realmente funcionam para B2B industrial, como montar lista de leads",
        "nao_falar": "NÃƒO dÃª conselhos vagos como 'invista em marketing'. DÃª PASSOS CONCRETOS com ferramentas e canais especÃ­ficos."
    },
    {
        "id": "presenca_online",
        "nome": "PresenÃ§a Online",
        "icone": "ðŸ“±",
        "cor": "#3b82f6",
        "foco": "palavras-chave que clientes desse nicho pesquisam no Google, exemplos de Google Ads funcionais, como usar LinkedIn para B2B industrial, tipo de conteÃºdo que gera leads nesse setor",
        "nao_falar": "NÃƒO fale sobre tarifas de importaÃ§Ã£o, feiras internacionais ou coisas nÃ£o relacionadas a marketing digital. Foque APENAS em aÃ§Ãµes online."
    },
    {
        "id": "precificacao",
        "nome": "PreÃ§os e Margens",
        "icone": "ðŸ’Ž",
        "cor": "#ec4899",
        "foco": "faixa de preÃ§o B2B/industrial (NÃƒO varejo), margem de lucro tÃ­pica do setor, modelos de contrato recorrente, como precificar para ser competitivo sem sacrificar margem",
        "nao_falar": "NÃƒO mostre preÃ§os de lojas como Kalunga ou Leroy Merlin (isso Ã© varejo). Foque em preÃ§os entre EMPRESAS (B2B)."
    },
]

def generate_business_queries(description, api_key):
    """Use Groq to generate targeted search queries for each business category."""
    categories_detail = ""
    for cat in BUSINESS_CATEGORIES:
        categories_detail += f'    "{cat["id"]}": {cat["foco"]} (CUIDADO: {cat["nao_falar"]})\n'
    
    prompt = f"""VocÃª Ã© um especialista em pesquisa de mercado B2B brasileiro.

Gere UMA query de busca para cada categoria baseada no negÃ³cio descrito.

REGRAS PARA AS QUERIES:
- 5 a 10 palavras cada
- Use o NOME TÃ‰CNICO do produto/serviÃ§o (ex: "cartonagem" ou "papelÃ£o ondulado", nÃ£o "embalagem")
- Inclua cidade/estado quando relevante
- Cada query deve buscar ALGO DIFERENTE â€” sem sobreposiÃ§Ã£o entre categorias
- publico_alvo: busque quem COMPRA esse produto (os clientes potenciais), NÃƒO quem fabrica
- precificacao: busque tabelas de preÃ§o INDUSTRIAIS/B2B, nÃ£o de varejo
- presenca_online: busque palavras-chave que COMPRADORES pesquisam quando precisam do produto
- como_vender: busque tÃ©cnicas de prospecÃ§Ã£o B2B para o setor INDUSTRIAL

NegÃ³cio:
"{description}"

Categorias:
{categories_detail}

JSON:
{{
    "queries": {{
        "mercado": "query",
        "concorrentes": "query",
        "publico_alvo": "query",
        "como_vender": "query",
        "presenca_online": "query",
        "precificacao": "query"
    }}
}}"""
    return call_groq(api_key, prompt, temperature=0.2)

def search_and_summarize_category(category, query, business_description, api_key, region, max_results=6, max_pages=2):
    """Search and summarize for a single business category."""
    print(f"  [{category['icone']}] Buscando: {query}", file=sys.stderr)
    
    results = search_duckduckgo(query, max_results=max_results, region=region)
    
    if not results:
        return {
            "id": category["id"],
            "nome": category["nome"],
            "icone": category["icone"],
            "cor": category["cor"],
            "query_usada": query,
            "resumo": {"info": "Nenhum resultado encontrado para esta categoria."},
            "fontes": []
        }
    
    aggregated_text = ""
    sources = []
    
    for i, result in enumerate(results):
        url = result.get('href', '')
        sources.append(url)
        snippet = result.get('body', '')
        title = result.get('title', '')
        aggregated_text += f"Fonte {i+1} ({title}): {snippet}\n"
        
        if i < max_pages:
            content = scrape_page(url)
            if content:
                aggregated_text += f"ConteÃºdo completo Fonte {i+1}: {content}\n"
    
    # Small delay to avoid rate limits
    time.sleep(2)
    
    try:
        prompt = f"""VocÃª Ã© um consultor sÃªnior de negÃ³cios. Analise dados reais da internet e gere um relatÃ³rio ÃšTIL.

O CLIENTE:
{business_description}

SEU FOCO NESTA SEÃ‡ÃƒO: {category['foco']}

REGRAS CRÃTICAS â€” LEIA ANTES DE RESPONDER:
1. Retorne APENAS JSON vÃ¡lido.
2. {category.get('nao_falar', '')}
3. NÃƒO REPITA o que o cliente jÃ¡ disse sobre o prÃ³prio negÃ³cio. Ele jÃ¡ sabe que faz consultoria tÃ©cnica, que atende B2B, etc. Traga informaÃ§Ãµes NOVAS que ele nÃ£o tem.
4. Fale em SEGUNDA PESSOA: "VocÃª pode...", "Seu mercado...", "Seus concorrentes...". Nunca diga "o cliente" ou "a empresa".
5. Cite nomes reais, valores em R$, percentuais â€” dados CONCRETOS dos textos abaixo. 
6. Se um dado nÃ£o existir nos textos, simplesmente NÃƒO inclua esse campo. NÃƒO escreva "dado nÃ£o disponÃ­vel".
7. CNPJ (XX.XXX.XXX/XXXX-XX) NÃƒO Ã© faturamento. Ignore CNPJs.
8. Cada recomendaÃ§Ã£o deve ser uma AÃ‡ÃƒO CONCRETA executÃ¡vel em 1-2 semanas, com nome de ferramenta/canal/empresa quando possÃ­vel.
9. NÃƒO repita recomendaÃ§Ãµes que jÃ¡ foram dadas em outras seÃ§Ãµes. Cada seÃ§Ã£o deve trazer VALOR ÃšNICO.

ESTRUTURA DO JSON:
{{
    "visao_geral": "2-3 frases com a principal conclusÃ£o NOVA para o cliente, sem repetir o que ele jÃ¡ sabe",
    "pontos_chave": [
        "Fato descoberto nos dados com nÃºmero ou nome concreto",
        "(mÃ­nimo 3, mÃ¡ximo 5 â€” sÃ³ inclua se for informaÃ§Ã£o NOVA e ÃšTIL)"
    ],
    "recomendacoes": [
        "AÃ§Ã£o concreta: o quÃª fazer + como + com qual ferramenta/canal (mÃ­nimo 2, mÃ¡ximo 4)"
    ],
    "dados_relevantes": {{
        "chave": "valor concreto encontrado nos dados (SÃ“ inclua se tiver valor real, NUNCA coloque 'dado nÃ£o disponÃ­vel')"
    }}
}}

DADOS DA INTERNET:
{aggregated_text[:18000]}"""
        
        resumo = call_groq(api_key, prompt, temperature=0.3)
    except Exception as e:
        print(f"  âŒ Erro ao resumir {category['nome']}: {e}", file=sys.stderr)
        resumo = {"erro": f"NÃ£o foi possÃ­vel gerar resumo: {str(e)[:200]}"}
    
    return {
        "id": category["id"],
        "nome": category["nome"],
        "icone": category["icone"],
        "cor": category["cor"],
        "query_usada": query,
        "resumo": resumo,
        "fontes": sources
    }

def run_business_analysis(args):
    """Run multi-category business intelligence analysis."""
    description = args.query
    api_key = os.environ.get("GROQ_API_KEY")
    
    if not api_key:
        return {
            "businessMode": True,
            "categories": [],
            "allSources": [],
            "erro": "Chave da API Groq nÃ£o configurada. Adicione GROQ_API_KEY no arquivo .env."
        }
    
    # Step 1: Generate search queries using AI
    print("ðŸ§  Gerando queries de busca inteligentes...", file=sys.stderr)
    try:
        query_result = generate_business_queries(description, api_key)
        queries = query_result.get("queries", {})
        print(f"  âœ… Queries geradas: {json.dumps(queries, ensure_ascii=False)}", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Erro ao gerar queries: {e}", file=sys.stderr)
        return {
            "businessMode": True,
            "categories": [],
            "allSources": [],
            "erro": f"Erro ao gerar queries de busca: {str(e)[:200]}"
        }
    
    # Step 2: Search and summarize SEQUENTIALLY to avoid rate limits
    print("ðŸ” Executando buscas e anÃ¡lises por categoria...", file=sys.stderr)
    categories_result = []
    all_sources = []
    
    for cat in BUSINESS_CATEGORIES:
        q = queries.get(cat["id"], f"{cat['nome']} {description[:50]}")
        try:
            result = search_and_summarize_category(
                cat, q, description, api_key, args.region,
                max_results=6, max_pages=2
            )
            categories_result.append(result)
            all_sources.extend(result.get("fontes", []))
        except Exception as e:
            print(f"  âŒ Erro na categoria {cat['id']}: {e}", file=sys.stderr)
            categories_result.append({
                "id": cat["id"],
                "nome": cat["nome"],
                "icone": cat["icone"],
                "cor": cat["cor"],
                "query_usada": q,
                "resumo": {"erro": f"Falha ao processar: {str(e)[:150]}"},
                "fontes": []
            })
    
    unique_sources = list(dict.fromkeys(all_sources))
    
    print(f"âœ… AnÃ¡lise completa! {len(categories_result)} categorias, {len(unique_sources)} fontes.", file=sys.stderr)
    
    return {
        "businessMode": True,
        "descricao": description,
        "categories": categories_result,
        "allSources": unique_sources
    }

# ==============================================================================
# Main
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="Search and Summarize CLI")
    parser.add_argument("query", help="Search query or business description")
    parser.add_argument("--max-results", type=int, default=8)
    parser.add_argument("--max-pages", type=int, default=3)
    parser.add_argument("--max-sentences", type=int, default=5)
    parser.add_argument("--region", type=str, default="br-pt")
    parser.add_argument("--business", action="store_true", help="Enable business intelligence mode")
    parser.add_argument("--list-sources", action="store_true")
    parser.add_argument("--no-groq", action="store_true")
    parser.add_argument("--verbose", action="store_true")

    args = parser.parse_args()

    try:
        sys.stdout.reconfigure(encoding='utf-8')
    except Exception:
        pass

    if args.business:
        output = run_business_analysis(args)
    else:
        output = run_simple_search(args)

    print("--- Resumo ---")
    print(json.dumps(output, indent=2, ensure_ascii=False))
    
    sources = output.get("sources", output.get("allSources", []))
    print("Fontes utilizadas:")
    for url in (sources or []):
        print(url)

if __name__ == "__main__":
    main()
